{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from random import random\n",
    "from random import seed\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function get the next row and colume based on the policy with probability 0.8\n",
    "#and perpendicular on the policy with probability 0.2\n",
    "\n",
    "#if the function will take the perpendicular direction\n",
    "#it will choose the either of the two perpendicular direction on 50% probability\n",
    "def get_next_r_c(r, c):\n",
    "    prob = random()\n",
    "    #80% of the time will take the policy action\n",
    "    if prob <= 0.8: \n",
    "        if policy[r][c] == 'up':\n",
    "            rnext = r - 1\n",
    "            cnext = c\n",
    "        elif policy[r][c] == 'left':\n",
    "            rnext = r\n",
    "            cnext = c - 1\n",
    "        elif policy[r][c] == 'right':\n",
    "            rnext = r\n",
    "            cnext = c + 1\n",
    "        elif policy[r][c] == 'down':\n",
    "            rnext = r + 1\n",
    "            cnext = c\n",
    "            \n",
    "    #20% of the time will make a perpendiclar action on the policy action\n",
    "    else:\n",
    "        if policy[r][c] == 'up' or policy[r][c] == 'down':\n",
    "            prob2 = random()\n",
    "            if prob2>0.5:\n",
    "                #left\n",
    "                rnext = r\n",
    "                cnext = c - 1\n",
    "            else:\n",
    "                #right\n",
    "                rnext = r\n",
    "                cnext = c + 1\n",
    "                \n",
    "        elif policy[r][c] == 'right' or policy[r][c] == 'left':\n",
    "            prob2 = random()\n",
    "            if prob2>0.5:\n",
    "                #up\n",
    "                rnext = r - 1\n",
    "                cnext = c\n",
    "            else:\n",
    "                #down\n",
    "                rnext = r + 1\n",
    "                cnext = c\n",
    "                \n",
    "    return rnext, cnext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in recursive_learning we start in random position given then we choose a path 0.8 on policy and 0.2 perpendicular on it\n",
    "#and keep recursing till we reach an exit cell, then we roll back and calculate the utility of each cell based on the value\n",
    "#of the exit cell and the one_step_rewardd\n",
    "\n",
    "#also we make sure when pick the next row and colume that the new position are not for abstacle or out of the world rang\n",
    "def recursive_learning(r, c):\n",
    "    if policy[r][c] == 'exit':\n",
    "        return Vs[r][c]\n",
    "    \n",
    "    rnext, cnext = get_next_r_c(r, c)\n",
    "    \n",
    "    while rnext >= rows or rnext < 0 or cnext >= cols or cnext < 0 or policy[rnext][cnext] == 'obstacle':\n",
    "        rnext, cnext = get_next_r_c(r, c)\n",
    "    \n",
    "    value = recursive_learning(rnext,cnext) + one_step_reward\n",
    "    \n",
    "    Vs[r][c] = Vs[r][c] + value\n",
    "    count[r][c] += 1\n",
    "    \n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in TRlearn we just pick a random position to start with and\n",
    "#make sure that the position is not for an obstacle or and exit cell\n",
    "def TRlearn():\n",
    "    r = randint(0, rows - 1)\n",
    "    c = randint(0, cols - 1)\n",
    "    while policy[r][c] == 'obstacle' or policy[r][c] == 'exit':\n",
    "        r = randint(0, rows - 1)\n",
    "        c = randint(0, cols - 1)\n",
    "    \n",
    "    recursive_learning(r, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    for i in range(iterations):\n",
    "        TRlearn()\n",
    "        \n",
    "    values = np.divide(Vs, count)\n",
    "                \n",
    "    print(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-dd44aec91226>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m#constructing the values matrix with zeros\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mVs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[0mVs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpositive_reward_position_row\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpositive_reward_position_col\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0mVs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnegative_reward_position_row\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnegative_reward_position_col\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "#number of iteration we need to learn\n",
    "iterations = 1000\n",
    "\n",
    "#should be read it from outside file\n",
    "#policy of 4*3 world (from the textbook)\n",
    "policy = [\n",
    "    ['right', 'right',    'right', 'exit'],\n",
    "    ['up',    'obstacle', 'up',    'exit'],\n",
    "    ['up',    'left',     'up',  'left']\n",
    "]\n",
    "\n",
    "#position of exit cells (positive and negative reward)\n",
    "positive_reward_position_row = 0\n",
    "positive_reward_position_col = 3\n",
    "negative_reward_position_row = 1\n",
    "negative_reward_position_col = 3\n",
    "\n",
    "#saving the world dimensions\n",
    "rows = len(policy)\n",
    "cols = len(policy[0]) \n",
    "\n",
    "#constructing the values matrix with zeros\n",
    "#and the exiting cells with its values (1 and -1)\n",
    "Vs = np.zeros((rows,cols))\n",
    "Vs[positive_reward_position_row][positive_reward_position_col] = 1\n",
    "Vs[negative_reward_position_row][negative_reward_position_col] = -1\n",
    "\n",
    "#reward of moving one step\n",
    "one_step_reward = -0.04\n",
    "\n",
    "#count matrix for counting the each time passed on certain cell for taking the average after taking all iterations\n",
    "#also setting the count for the exit cells to one so its values doesn't change after taking the average.\n",
    "count = np.zeros((rows, cols))\n",
    "count[positive_reward_position_row][positive_reward_position_col] = 1\n",
    "count[negative_reward_position_row][negative_reward_position_col] = 1\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.59636364 0.62909091 0.66742857 0.712      0.74461538 0.79882353\n",
      "  0.84571429 0.89666667 0.94819048 1.        ]\n",
      " [0.51384615 0.56296296 0.6097561  0.65632653 0.70793651 0.75287671\n",
      "  0.80764045 0.85567251 0.90087513 0.94919868]\n",
      " [0.464      0.51666667 0.5775     0.61435897 0.6662069  0.69671642\n",
      "  0.74626866 0.80201592 0.85214374 0.89964497]\n",
      " [0.39733333 0.48666667 0.51619048 0.55854545 0.615      0.66415094\n",
      "  0.70569444 0.75330422 0.7986755  0.85061224]\n",
      " [0.35555556 0.40869565 0.45853659 0.5047619  0.55752577 0.60339623\n",
      "  0.65322581 0.70028986 0.74024096 0.80307692]\n",
      " [0.33714286 0.29454545 0.37371429 0.4672     0.50306189 0.55072464\n",
      "  0.60505263 0.65223529 0.67888889 0.75267606]\n",
      " [0.335      0.31666667 0.37142857 0.39671498 0.45289796 0.5047619\n",
      "  0.55259259 0.60657534 0.62596491 0.7077551 ]\n",
      " [0.12       0.25230769 0.30774194 0.35291139 0.44627451 0.46634146\n",
      "  0.50918919 0.5552     0.57209302 0.65176471]\n",
      " [0.13538462 0.21931034 0.25178947 0.2969697  0.4        0.42222222\n",
      "  0.45538462 0.50571429 0.52       0.59      ]\n",
      " [0.12       0.18933333 0.216      0.26461538 0.35       0.38153846\n",
      "  0.44       0.43294118 0.44       0.56      ]]\n"
     ]
    }
   ],
   "source": [
    "#policy of 4*3 world (from the textbook)\n",
    "policy = [\n",
    "    ['right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'exit' ],\n",
    "    ['right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'up'   ],\n",
    "    ['right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'up'   , 'up'   ],\n",
    "    ['right', 'right', 'right', 'right', 'right', 'right', 'right', 'up',    'up'   , 'up'   ],\n",
    "    ['right', 'right', 'right', 'right', 'right', 'right', 'up'   , 'up',    'up'   , 'up'   ],\n",
    "    ['right', 'right', 'right', 'right', 'right', 'up'   , 'up'   , 'up',    'up'   , 'up'   ],\n",
    "    ['right', 'right', 'right', 'right', 'up'   , 'up'   , 'up'   , 'up',    'up'   , 'up'   ],\n",
    "    ['right', 'right', 'right', 'up'   , 'up'   , 'up'   , 'up'   , 'up',    'up'   , 'up'   ],\n",
    "    ['right', 'right', 'up'   , 'up'   , 'up'   , 'up'   , 'up'   , 'up',    'up'   , 'up'   ],\n",
    "    ['right', 'up',    'up'   , 'up'   , 'up'   , 'up'   , 'up'   , 'up',    'up'   , 'up'   ]\n",
    "]\n",
    "\n",
    "#position of exit cells (positive reward)\n",
    "positive_reward_position_row = 0\n",
    "positive_reward_position_col = 9\n",
    "\n",
    "#saving the world dimensions\n",
    "rows = len(policy)\n",
    "cols = len(policy[0]) \n",
    "\n",
    "#constructing the values matrix with zeros\n",
    "#and the exiting cell with its value (1)\n",
    "Vs = np.zeros((rows,cols))\n",
    "Vs[positive_reward_position_row][positive_reward_position_col] = 1\n",
    "\n",
    "#reward of moving one step\n",
    "one_step_reward = -0.04\n",
    "\n",
    "#count matrix for counting the each time passed on certain cell for taking the average after taking all iterations\n",
    "#also setting the count for the exit cell to one so its value doesn't change after taking the average.\n",
    "count = np.zeros((rows, cols))\n",
    "count[positive_reward_position_row][positive_reward_position_col] = 1\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.60941176 0.71111111 0.71636364 0.74666667 0.80631579 0.72888889\n",
      "  0.70181818 0.65777778 0.57333333 0.56      ]\n",
      " [0.64       0.69454545 0.73111111 0.77       0.83885714 0.776\n",
      "  0.7225     0.68757895 0.64       0.57714286]\n",
      " [0.68941176 0.73552239 0.78836364 0.85463415 0.89584906 0.832\n",
      "  0.7805     0.73419355 0.696      0.66857143]\n",
      " [0.728      0.77290323 0.83414013 0.88875    0.9424     0.88876712\n",
      "  0.83287671 0.78461538 0.728      0.64307692]\n",
      " [0.77066667 0.83714286 0.87125    0.93781513 1.         0.93918919\n",
      "  0.868      0.808      0.7376     0.65882353]\n",
      " [0.704      0.78461538 0.84857143 0.88851986 0.93684887 0.89207756\n",
      "  0.83717842 0.774      0.73333333 0.66222222]\n",
      " [0.71294118 0.73641026 0.77129412 0.83251142 0.87172414 0.85\n",
      "  0.79256281 0.74503401 0.69081081 0.65066667]\n",
      " [0.65       0.67906977 0.7292562  0.78926829 0.81894737 0.80285714\n",
      "  0.72355556 0.68067797 0.61741176 0.552     ]\n",
      " [0.56       0.61859155 0.68       0.74666667 0.768      0.75294118\n",
      "  0.65142857 0.66933333 0.56698413 0.50536585]\n",
      " [0.51555556 0.57176471 0.65142857 0.672      0.70545455 0.696\n",
      "  0.64       0.65       0.48       0.472     ]]\n"
     ]
    }
   ],
   "source": [
    "policy = [\n",
    "    ['down', 'down'  , 'down' , 'down' , 'down', 'down', 'down', 'down', 'down', 'left'],\n",
    "    ['right', 'down' , 'down' , 'down' , 'down', 'down', 'down', 'down', 'left', 'left'],\n",
    "    ['right', 'right', 'down' , 'down' , 'down', 'down', 'down', 'left', 'left', 'left'],\n",
    "    ['right', 'right', 'right', 'down' , 'down', 'down', 'left', 'left', 'left', 'left'],\n",
    "    ['right', 'right', 'right', 'right', 'exit', 'left', 'left', 'left', 'left', 'left'],\n",
    "    ['right', 'right', 'right', 'right', 'up'  , 'up'  , 'left', 'left', 'left', 'left'],\n",
    "    ['right', 'right', 'right', 'up'   , 'up'  , 'up'  , 'up'  , 'left', 'left', 'left'],\n",
    "    ['right', 'right', 'up'   , 'up'   , 'up'  , 'up'  , 'up'  , 'up'  , 'left', 'left'],\n",
    "    ['right', 'up'   , 'up'   , 'up'   , 'up'  , 'up'  , 'up'  , 'up'  , 'up'  , 'left'],\n",
    "    ['up'   , 'up'   , 'up'   , 'up'   , 'up'  , 'up'  , 'up'  , 'up'  , 'up'  , 'up'  ]\n",
    "]\n",
    "\n",
    "positive_reward_position_row = 4\n",
    "positive_reward_position_col = 4\n",
    "\n",
    "rows = len(policy)\n",
    "cols = len(policy[0]) \n",
    "\n",
    "Vs = np.zeros((rows,cols))\n",
    "Vs[positive_reward_position_row][positive_reward_position_col] = 1\n",
    "\n",
    "one_step_reward = -0.04\n",
    " \n",
    "count = np.zeros((rows, cols))\n",
    "count[positive_reward_position_row][positive_reward_position_col] = 1\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
